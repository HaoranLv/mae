{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch BYOS\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments.  This can speed up iterative testing and debugging while using the same familiar Python SDK interface.  Just change your estimator's `train_instance_type` to `local` (or `local_gpu` if you're using an ml.p2 or ml.p3 notebook instance).\n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU).\n",
    "\n",
    "**Note, you can only run a single local notebook at one time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./utils/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **SageMaker Python SDK** helps you deploy your models for training and hosting in optimized, productions ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow, MXNet, PyTorch and Chainer. This tutorial focuses on how to create a convolutional neural network model to train the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) using **PyTorch in local mode**.\n",
    "\n",
    "### Set up the environment\n",
    "\n",
    "This notebook was created and tested on a single ml.p2.xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mae'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 18 04:47:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   41C    P0    39W / 300W |   6382MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2112      C   ...vs/pytorch_p37/bin/python     4075MiB |\n",
      "|    0   N/A  N/A     14267      C   ...vs/pytorch_p37/bin/python     2305MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'\n",
    "    \n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained model & Training data\n",
    "\n",
    "You can also prepare your own image classification dataset, please organize it in the following format:\n",
    "\n",
    "Dataset_customer/\n",
    "\n",
    "    train/\n",
    "        class1/\n",
    "            0.jpg\n",
    "            1.jpg\n",
    "            ...\n",
    "        class2/\n",
    "        ...\n",
    "    val/\n",
    "        class1/\n",
    "            0.jpg\n",
    "            1.jpg\n",
    "            ...\n",
    "        class2/\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-18 03:22:02--  https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 343249461 (327M) [binary/octet-stream]\n",
      "Saving to: ‘mae_pretrain_vit_base.pth’\n",
      "\n",
      "100%[======================================>] 343,249,461 40.2MB/s   in 7.9s   \n",
      "\n",
      "2022-03-18 03:22:11 (41.3 MB/s) - ‘mae_pretrain_vit_base.pth’ saved [343249461/343249461]\n",
      "\n",
      "download: s3://datalab2022/chinesefoodnet/ChineseFoodNet.zip to datasets/ChineseFoodNet.zip\n"
     ]
    }
   ],
   "source": [
    "!cd pretrained_model && wget -nc https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth\n",
    "!aws s3 cp s3://datalab2022/chinesefoodnet/chinesefoodnet_small.zip ./datasets\n",
    "!cd datasets && unzip chinesefoodnet_small.zip > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': 'file:///home/ec2-user/SageMaker/mae/pretrained_model/', 'data_path': 'file:///home/ec2-user/SageMaker/mae/datasets/'}\n"
     ]
    }
   ],
   "source": [
    "WORK_DIRECTORY1 ='./pretrained_model'\n",
    "WORK_DIRECTORY2 ='./datasets'\n",
    "print('s3://{}/{}'.format(bucket, prefix))\n",
    "data_location = sagemaker_session.upload_data(WORK_DIRECTORY1, key_prefix=prefix)\n",
    "print(data_location)\n",
    "data_location = sagemaker_session.upload_data(WORK_DIRECTORY2, key_prefix=prefix)\n",
    "data_location = 's3://{}/{}'.format(bucket, prefix)\n",
    "print(data_location)\n",
    "inputs = {'weights': data_location+'/pretrained_model/', 'data_path':data_location+'/datasets/'}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Functions\n",
    "\n",
    "SageMaker invokes the main function defined within your training script for training. When deploying your trained model to an endpoint, the model_fn() is called to determine how to load your trained model. The model_fn() along with a few other functions list below are called to enable predictions on SageMaker.\n",
    "\n",
    "### [Predicting Functions](https://github.com/aws/sagemaker-pytorch-containers/blob/master/src/sagemaker_pytorch_container/serving.py)\n",
    "* model_fn(model_dir) - loads your model.\n",
    "* input_fn(serialized_input_data, content_type) - deserializes predictions to predict_fn.\n",
    "* output_fn(prediction_output, accept) - serializes predictions from predict_fn.\n",
    "* predict_fn(input_data, model) - calls a model on data deserialized in input_fn.\n",
    "\n",
    "The model_fn() is the only function that doesn't have a default implementation and is required by the user for using PyTorch on SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.PyTorch estimator\n",
    "\n",
    "The `PyTorch` class allows us to run our training function on SageMaker. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. For local training with GPU, we could set this to \"local_gpu\".  In this case, `instance_type` was set above based on your whether you're running a GPU instance.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training using GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmp27ttpd_j'...\n",
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/main'.\n",
      "2022-03-18 05:39:31 Starting - Starting the training job...\n",
      "2022-03-18 05:39:58 Starting - Preparing the instances for trainingProfilerReport-1647581971: InProgress\n",
      ".........\n",
      "2022-03-18 05:41:17 Downloading - Downloading input data...\n",
      "2022-03-18 05:41:54 Training - Downloading the training image....................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-03-18 05:45:07,003 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-03-18 05:45:07,028 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-03-18 05:45:07,037 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-03-18 05:45:07,499 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting timm==0.3.2\n",
      "  Downloading timm-0.3.2-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.6/site-packages (from timm==0.3.2->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from timm==0.3.2->-r requirements.txt (line 1)) (0.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.0->timm==0.3.2->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.0->timm==0.3.2->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch>=1.0->timm==0.3.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.35.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->timm==0.3.2->-r requirements.txt (line 1)) (8.2.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, timm, tensorboard\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.0.1\n",
      "    Uninstalling importlib-metadata-4.0.1:\n",
      "      Successfully uninstalled importlib-metadata-4.0.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.0.0 cachetools-4.2.4 google-auth-2.6.2 google-auth-oauthlib-0.4.6 grpcio-1.44.0 importlib-metadata-4.8.3 markdown-3.3.6 oauthlib-3.2.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 timm-0.3.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-03-18 05:45:14,973 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"data_path\": \"/opt/ml/input/data/data_path\",\n",
      "        \"weights\": \"/opt/ml/input/data/weights\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"accum_iter\": 4,\n",
      "        \"batch_size\": 32,\n",
      "        \"blr\": 0.0005,\n",
      "        \"cutmix\": 1.0,\n",
      "        \"data_path\": \"/opt/ml/input/data/data_path/chinesefoodnet_small\",\n",
      "        \"drop_path\": 0.1,\n",
      "        \"epochs\": 1,\n",
      "        \"finetune\": \"/opt/ml/input/data/weights/mae_pretrain_vit_base.pth\",\n",
      "        \"layer_decay\": 0.65,\n",
      "        \"mixup\": 0.8,\n",
      "        \"model\": \"vit_base_patch16\",\n",
      "        \"output_dir\": \"/opt/ml/model/\",\n",
      "        \"reprob\": 0.25,\n",
      "        \"weight_decay\": 0.05\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"data_path\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-03-18-05-39-29-920\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-847380964353/pytorch-training-2022-03-18-05-39-29-920/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"main_finetune\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"main_finetune.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"accum_iter\":4,\"batch_size\":32,\"blr\":0.0005,\"cutmix\":1.0,\"data_path\":\"/opt/ml/input/data/data_path/chinesefoodnet_small\",\"drop_path\":0.1,\"epochs\":1,\"finetune\":\"/opt/ml/input/data/weights/mae_pretrain_vit_base.pth\",\"layer_decay\":0.65,\"mixup\":0.8,\"model\":\"vit_base_patch16\",\"output_dir\":\"/opt/ml/model/\",\"reprob\":0.25,\"weight_decay\":0.05}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=main_finetune.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"data_path\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"weights\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"data_path\",\"weights\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=main_finetune\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-847380964353/pytorch-training-2022-03-18-05-39-29-920/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"data_path\":\"/opt/ml/input/data/data_path\",\"weights\":\"/opt/ml/input/data/weights\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"accum_iter\":4,\"batch_size\":32,\"blr\":0.0005,\"cutmix\":1.0,\"data_path\":\"/opt/ml/input/data/data_path/chinesefoodnet_small\",\"drop_path\":0.1,\"epochs\":1,\"finetune\":\"/opt/ml/input/data/weights/mae_pretrain_vit_base.pth\",\"layer_decay\":0.65,\"mixup\":0.8,\"model\":\"vit_base_patch16\",\"output_dir\":\"/opt/ml/model/\",\"reprob\":0.25,\"weight_decay\":0.05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data_path\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"weights\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-03-18-05-39-29-920\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-847380964353/pytorch-training-2022-03-18-05-39-29-920/source/sourcedir.tar.gz\",\"module_name\":\"main_finetune\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"main_finetune.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--accum_iter\",\"4\",\"--batch_size\",\"32\",\"--blr\",\"0.0005\",\"--cutmix\",\"1.0\",\"--data_path\",\"/opt/ml/input/data/data_path/chinesefoodnet_small\",\"--drop_path\",\"0.1\",\"--epochs\",\"1\",\"--finetune\",\"/opt/ml/input/data/weights/mae_pretrain_vit_base.pth\",\"--layer_decay\",\"0.65\",\"--mixup\",\"0.8\",\"--model\",\"vit_base_patch16\",\"--output_dir\",\"/opt/ml/model/\",\"--reprob\",\"0.25\",\"--weight_decay\",\"0.05\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATA_PATH=/opt/ml/input/data/data_path\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_WEIGHTS=/opt/ml/input/data/weights\u001b[0m\n",
      "\u001b[34mSM_HP_ACCUM_ITER=4\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_BLR=0.0005\u001b[0m\n",
      "\u001b[34mSM_HP_CUTMIX=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_PATH=/opt/ml/input/data/data_path/chinesefoodnet_small\u001b[0m\n",
      "\u001b[34mSM_HP_DROP_PATH=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_FINETUNE=/opt/ml/input/data/weights/mae_pretrain_vit_base.pth\u001b[0m\n",
      "\u001b[34mSM_HP_LAYER_DECAY=0.65\u001b[0m\n",
      "\u001b[34mSM_HP_MIXUP=0.8\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL=vit_base_patch16\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model/\u001b[0m\n",
      "\u001b[34mSM_HP_REPROB=0.25\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.05\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 main_finetune.py --accum_iter 4 --batch_size 32 --blr 0.0005 --cutmix 1.0 --data_path /opt/ml/input/data/data_path/chinesefoodnet_small --drop_path 0.1 --epochs 1 --finetune /opt/ml/input/data/weights/mae_pretrain_vit_base.pth --layer_decay 0.65 --mixup 0.8 --model vit_base_patch16 --output_dir /opt/ml/model/ --reprob 0.25 --weight_decay 0.05\u001b[0m\n",
      "\u001b[34mNot using distributed mode\u001b[0m\n",
      "\u001b[34m[05:45:17.494264] job dir: /opt/ml/code\u001b[0m\n",
      "\u001b[34m[05:45:17.494364] Namespace(aa='rand-m9-mstd0.5-inc1',\u001b[0m\n",
      "\u001b[34maccum_iter=4,\u001b[0m\n",
      "\u001b[34mbatch_size=32,\u001b[0m\n",
      "\u001b[34mblr=0.0005,\u001b[0m\n",
      "\u001b[34mclip_grad=None,\u001b[0m\n",
      "\u001b[34mcolor_jitter=None,\u001b[0m\n",
      "\u001b[34mcutmix=1.0,\u001b[0m\n",
      "\u001b[34mcutmix_minmax=None,\u001b[0m\n",
      "\u001b[34mdata_path='/opt/ml/input/data/data_path/chinesefoodnet_small',\u001b[0m\n",
      "\u001b[34mdevice='cuda',\u001b[0m\n",
      "\u001b[34mdist_eval=False,\u001b[0m\n",
      "\u001b[34mdist_on_itp=False,\u001b[0m\n",
      "\u001b[34mdist_url='env://',\u001b[0m\n",
      "\u001b[34mdistributed=False,\u001b[0m\n",
      "\u001b[34mdrop_path=0.1,\u001b[0m\n",
      "\u001b[34mepochs=1,\u001b[0m\n",
      "\u001b[34meval=False,\u001b[0m\n",
      "\u001b[34mfinetune='/opt/ml/input/data/weights/mae_pretrain_vit_base.pth',\u001b[0m\n",
      "\u001b[34mglobal_pool=True,\u001b[0m\n",
      "\u001b[34minput_size=224,\u001b[0m\n",
      "\u001b[34mlayer_decay=0.65,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_dir='./output_dir',\u001b[0m\n",
      "\u001b[34mlr=None,\u001b[0m\n",
      "\u001b[34mmin_lr=1e-06,\u001b[0m\n",
      "\u001b[34mmixup=0.8,\u001b[0m\n",
      "\u001b[34mmixup_mode='batch',\u001b[0m\n",
      "\u001b[34mmixup_prob=1.0,\u001b[0m\n",
      "\u001b[34mmixup_switch_prob=0.5,\u001b[0m\n",
      "\u001b[34mmodel='vit_base_patch16',\u001b[0m\n",
      "\u001b[34mnb_classes=1000,\u001b[0m\n",
      "\u001b[34mnum_workers=10,\u001b[0m\n",
      "\u001b[34moutput_dir='/opt/ml/model/',\u001b[0m\n",
      "\u001b[34mpin_mem=True,\u001b[0m\n",
      "\u001b[34mrecount=1,\u001b[0m\n",
      "\u001b[34mremode='pixel',\u001b[0m\n",
      "\u001b[34mreprob=0.25,\u001b[0m\n",
      "\u001b[34mresplit=False,\u001b[0m\n",
      "\u001b[34mresume='',\u001b[0m\n",
      "\u001b[34msave_last_only=True,\u001b[0m\n",
      "\u001b[34mseed=0,\u001b[0m\n",
      "\u001b[34msmoothing=0.1,\u001b[0m\n",
      "\u001b[34mstart_epoch=0,\u001b[0m\n",
      "\u001b[34mwarmup_epochs=5,\u001b[0m\n",
      "\u001b[34mweight_decay=0.05,\u001b[0m\n",
      "\u001b[34mworld_size=1)\u001b[0m\n",
      "\u001b[34m[05:45:17.512458] Dataset ImageFolder\n",
      "    Number of datapoints: 1616\n",
      "    Root location: /opt/ml/input/data/data_path/chinesefoodnet_small/train\n",
      "    StandardTransform\u001b[0m\n",
      "\u001b[34mTransform: Compose(\n",
      "               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               <timm.data.auto_augment.RandAugment object at 0x7f21f1ab1ac8>\n",
      "               ToTensor()\n",
      "               Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      "               <timm.data.random_erasing.RandomErasing object at 0x7f21f1ab1b38>\n",
      "           )\u001b[0m\n",
      "\u001b[34m[05:45:17.568714] Dataset ImageFolder\n",
      "    Number of datapoints: 226\n",
      "    Root location: /opt/ml/input/data/data_path/chinesefoodnet_small/val\n",
      "    StandardTransform\u001b[0m\n",
      "\u001b[34mTransform: Compose(\n",
      "               Resize(size=256, interpolation=PIL.Image.BICUBIC)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "           )\u001b[0m\n",
      "\u001b[34m[05:45:17.568837] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f21f1ab14a8>\u001b[0m\n",
      "\u001b[34m[05:45:17.570365] Mixup is activated!\u001b[0m\n",
      "\n",
      "2022-03-18 05:45:15 Training - Training image download completed. Training in progress.\u001b[34m[05:45:20.675321] Load pre-trained checkpoint from: /opt/ml/input/data/weights/mae_pretrain_vit_base.pth\u001b[0m\n",
      "\u001b[34m[05:45:20.732262] _IncompatibleKeys(missing_keys=['head.weight', 'head.bias', 'fc_norm.weight', 'fc_norm.bias'], unexpected_keys=['norm.weight', 'norm.bias'])\u001b[0m\n",
      "\u001b[34m[05:45:25.018415] Model = VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m[05:45:25.018489] number of params (M): 86.57\u001b[0m\n",
      "\u001b[34m[05:45:25.018524] base lr: 5.00e-04\u001b[0m\n",
      "\u001b[34m[05:45:25.018552] actual lr: 2.50e-04\u001b[0m\n",
      "\u001b[34m[05:45:25.018578] accumulate grad iterations: 4\u001b[0m\n",
      "\u001b[34m[05:45:25.018610] effective batch size: 128\u001b[0m\n",
      "\u001b[34m[05:45:25.028935] criterion = SoftTargetCrossEntropy()\u001b[0m\n",
      "\u001b[34m[05:45:25.028985] Start training for 1 epochs\u001b[0m\n",
      "\u001b[34m[05:45:25.030333] log_dir: ./output_dir\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:25.427 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:25.500 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:25.501 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:25.501 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:25.502 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:25.502 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.434 algo-1:32 INFO hook.py:591] name:cls_token count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.435 algo-1:32 INFO hook.py:591] name:pos_embed count_params:151296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.435 algo-1:32 INFO hook.py:591] name:patch_embed.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.435 algo-1:32 INFO hook.py:591] name:patch_embed.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.435 algo-1:32 INFO hook.py:591] name:blocks.0.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.435 algo-1:32 INFO hook.py:591] name:blocks.0.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.435 algo-1:32 INFO hook.py:591] name:blocks.0.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.0.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.1.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.436 algo-1:32 INFO hook.py:591] name:blocks.1.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.437 algo-1:32 INFO hook.py:591] name:blocks.1.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.443 algo-1:32 INFO hook.py:591] name:blocks.1.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.444 algo-1:32 INFO hook.py:591] name:blocks.1.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.444 algo-1:32 INFO hook.py:591] name:blocks.1.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.444 algo-1:32 INFO hook.py:591] name:blocks.1.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.444 algo-1:32 INFO hook.py:591] name:blocks.1.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.444 algo-1:32 INFO hook.py:591] name:blocks.1.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.452 algo-1:32 INFO hook.py:591] name:blocks.1.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.452 algo-1:32 INFO hook.py:591] name:blocks.1.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.453 algo-1:32 INFO hook.py:591] name:blocks.1.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.453 algo-1:32 INFO hook.py:591] name:blocks.2.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.453 algo-1:32 INFO hook.py:591] name:blocks.2.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.453 algo-1:32 INFO hook.py:591] name:blocks.2.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.453 algo-1:32 INFO hook.py:591] name:blocks.2.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.454 algo-1:32 INFO hook.py:591] name:blocks.2.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.454 algo-1:32 INFO hook.py:591] name:blocks.2.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.454 algo-1:32 INFO hook.py:591] name:blocks.2.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.454 algo-1:32 INFO hook.py:591] name:blocks.2.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.454 algo-1:32 INFO hook.py:591] name:blocks.2.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.455 algo-1:32 INFO hook.py:591] name:blocks.2.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.455 algo-1:32 INFO hook.py:591] name:blocks.2.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.455 algo-1:32 INFO hook.py:591] name:blocks.2.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.456 algo-1:32 INFO hook.py:591] name:blocks.3.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.456 algo-1:32 INFO hook.py:591] name:blocks.3.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.456 algo-1:32 INFO hook.py:591] name:blocks.3.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.456 algo-1:32 INFO hook.py:591] name:blocks.3.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.457 algo-1:32 INFO hook.py:591] name:blocks.3.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.457 algo-1:32 INFO hook.py:591] name:blocks.3.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.458 algo-1:32 INFO hook.py:591] name:blocks.3.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.458 algo-1:32 INFO hook.py:591] name:blocks.3.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.458 algo-1:32 INFO hook.py:591] name:blocks.3.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.458 algo-1:32 INFO hook.py:591] name:blocks.3.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.458 algo-1:32 INFO hook.py:591] name:blocks.3.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.459 algo-1:32 INFO hook.py:591] name:blocks.3.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.459 algo-1:32 INFO hook.py:591] name:blocks.4.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.459 algo-1:32 INFO hook.py:591] name:blocks.4.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.459 algo-1:32 INFO hook.py:591] name:blocks.4.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.459 algo-1:32 INFO hook.py:591] name:blocks.4.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.460 algo-1:32 INFO hook.py:591] name:blocks.4.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.460 algo-1:32 INFO hook.py:591] name:blocks.4.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.460 algo-1:32 INFO hook.py:591] name:blocks.4.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.460 algo-1:32 INFO hook.py:591] name:blocks.4.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.461 algo-1:32 INFO hook.py:591] name:blocks.4.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.461 algo-1:32 INFO hook.py:591] name:blocks.4.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.461 algo-1:32 INFO hook.py:591] name:blocks.4.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.461 algo-1:32 INFO hook.py:591] name:blocks.4.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.462 algo-1:32 INFO hook.py:591] name:blocks.5.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.462 algo-1:32 INFO hook.py:591] name:blocks.5.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.462 algo-1:32 INFO hook.py:591] name:blocks.5.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.462 algo-1:32 INFO hook.py:591] name:blocks.5.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.462 algo-1:32 INFO hook.py:591] name:blocks.5.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.462 algo-1:32 INFO hook.py:591] name:blocks.5.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.463 algo-1:32 INFO hook.py:591] name:blocks.5.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.463 algo-1:32 INFO hook.py:591] name:blocks.5.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.463 algo-1:32 INFO hook.py:591] name:blocks.5.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.464 algo-1:32 INFO hook.py:591] name:blocks.5.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.464 algo-1:32 INFO hook.py:591] name:blocks.5.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.465 algo-1:32 INFO hook.py:591] name:blocks.5.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.466 algo-1:32 INFO hook.py:591] name:blocks.6.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.466 algo-1:32 INFO hook.py:591] name:blocks.6.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.466 algo-1:32 INFO hook.py:591] name:blocks.6.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.466 algo-1:32 INFO hook.py:591] name:blocks.6.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.467 algo-1:32 INFO hook.py:591] name:blocks.6.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.467 algo-1:32 INFO hook.py:591] name:blocks.6.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.468 algo-1:32 INFO hook.py:591] name:blocks.6.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.468 algo-1:32 INFO hook.py:591] name:blocks.6.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.468 algo-1:32 INFO hook.py:591] name:blocks.6.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.469 algo-1:32 INFO hook.py:591] name:blocks.6.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.469 algo-1:32 INFO hook.py:591] name:blocks.6.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.469 algo-1:32 INFO hook.py:591] name:blocks.6.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.470 algo-1:32 INFO hook.py:591] name:blocks.7.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.470 algo-1:32 INFO hook.py:591] name:blocks.7.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.472 algo-1:32 INFO hook.py:591] name:blocks.7.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.472 algo-1:32 INFO hook.py:591] name:blocks.7.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.472 algo-1:32 INFO hook.py:591] name:blocks.7.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.472 algo-1:32 INFO hook.py:591] name:blocks.7.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.473 algo-1:32 INFO hook.py:591] name:blocks.7.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.473 algo-1:32 INFO hook.py:591] name:blocks.7.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.473 algo-1:32 INFO hook.py:591] name:blocks.7.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.474 algo-1:32 INFO hook.py:591] name:blocks.7.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.474 algo-1:32 INFO hook.py:591] name:blocks.7.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.474 algo-1:32 INFO hook.py:591] name:blocks.7.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.477 algo-1:32 INFO hook.py:591] name:blocks.8.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.477 algo-1:32 INFO hook.py:591] name:blocks.8.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.478 algo-1:32 INFO hook.py:591] name:blocks.8.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.478 algo-1:32 INFO hook.py:591] name:blocks.8.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.478 algo-1:32 INFO hook.py:591] name:blocks.8.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.478 algo-1:32 INFO hook.py:591] name:blocks.8.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.478 algo-1:32 INFO hook.py:591] name:blocks.8.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.479 algo-1:32 INFO hook.py:591] name:blocks.8.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.479 algo-1:32 INFO hook.py:591] name:blocks.8.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.479 algo-1:32 INFO hook.py:591] name:blocks.8.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.479 algo-1:32 INFO hook.py:591] name:blocks.8.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.480 algo-1:32 INFO hook.py:591] name:blocks.8.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.480 algo-1:32 INFO hook.py:591] name:blocks.9.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.480 algo-1:32 INFO hook.py:591] name:blocks.9.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.480 algo-1:32 INFO hook.py:591] name:blocks.9.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.480 algo-1:32 INFO hook.py:591] name:blocks.9.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.481 algo-1:32 INFO hook.py:591] name:blocks.9.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.481 algo-1:32 INFO hook.py:591] name:blocks.9.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.481 algo-1:32 INFO hook.py:591] name:blocks.9.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.482 algo-1:32 INFO hook.py:591] name:blocks.9.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.482 algo-1:32 INFO hook.py:591] name:blocks.9.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.486 algo-1:32 INFO hook.py:591] name:blocks.9.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.487 algo-1:32 INFO hook.py:591] name:blocks.9.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.487 algo-1:32 INFO hook.py:591] name:blocks.9.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.487 algo-1:32 INFO hook.py:591] name:blocks.10.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.488 algo-1:32 INFO hook.py:591] name:blocks.10.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.488 algo-1:32 INFO hook.py:591] name:blocks.10.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.488 algo-1:32 INFO hook.py:591] name:blocks.10.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.488 algo-1:32 INFO hook.py:591] name:blocks.10.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.488 algo-1:32 INFO hook.py:591] name:blocks.10.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.489 algo-1:32 INFO hook.py:591] name:blocks.10.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.489 algo-1:32 INFO hook.py:591] name:blocks.10.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.489 algo-1:32 INFO hook.py:591] name:blocks.10.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.490 algo-1:32 INFO hook.py:591] name:blocks.10.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.490 algo-1:32 INFO hook.py:591] name:blocks.10.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.490 algo-1:32 INFO hook.py:591] name:blocks.10.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.490 algo-1:32 INFO hook.py:591] name:blocks.11.norm1.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.491 algo-1:32 INFO hook.py:591] name:blocks.11.norm1.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.491 algo-1:32 INFO hook.py:591] name:blocks.11.attn.qkv.weight count_params:1769472\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.491 algo-1:32 INFO hook.py:591] name:blocks.11.attn.qkv.bias count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.491 algo-1:32 INFO hook.py:591] name:blocks.11.attn.proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.492 algo-1:32 INFO hook.py:591] name:blocks.11.attn.proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.492 algo-1:32 INFO hook.py:591] name:blocks.11.norm2.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.492 algo-1:32 INFO hook.py:591] name:blocks.11.norm2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.492 algo-1:32 INFO hook.py:591] name:blocks.11.mlp.fc1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.492 algo-1:32 INFO hook.py:591] name:blocks.11.mlp.fc1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.493 algo-1:32 INFO hook.py:591] name:blocks.11.mlp.fc2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.493 algo-1:32 INFO hook.py:591] name:blocks.11.mlp.fc2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.493 algo-1:32 INFO hook.py:591] name:head.weight count_params:768000\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.494 algo-1:32 INFO hook.py:591] name:head.bias count_params:1000\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.495 algo-1:32 INFO hook.py:591] name:fc_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.496 algo-1:32 INFO hook.py:591] name:fc_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.496 algo-1:32 INFO hook.py:593] Total Trainable Params: 86567656\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.497 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-03-18 05:45:27.507 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m[05:45:36.296201] Epoch: [0]  [ 0/50]  eta: 0:09:23  lr: 0.000000  loss: 6.9075 (6.9075)  time: 11.2628  data: 2.3593  max mem: 4467\u001b[0m\n",
      "\u001b[34m[05:45:39.053903] Epoch: [0]  [20/50]  eta: 0:00:20  lr: 0.000020  loss: 6.9075 (6.9043)  time: 0.1378  data: 0.0002  max mem: 5464\u001b[0m\n",
      "\u001b[34m[05:45:41.737306] Epoch: [0]  [40/50]  eta: 0:00:04  lr: 0.000040  loss: 6.8708 (6.8855)  time: 0.1341  data: 0.0002  max mem: 5464\u001b[0m\n",
      "\u001b[34m[05:45:42.876514] Epoch: [0]  [49/50]  eta: 0:00:00  lr: 0.000048  loss: 6.8252 (6.8709)  time: 0.1295  data: 0.0001  max mem: 5464\u001b[0m\n",
      "\u001b[34m[05:45:42.945349] Epoch: [0] Total time: 0:00:17 (0.3583 s / it)\u001b[0m\n",
      "\u001b[34m[05:45:42.945660] Averaged stats: lr: 0.000048  loss: 6.8252 (6.8709)\u001b[0m\n",
      "\u001b[34m[05:45:45.916668] Test:  [0/8]  eta: 0:00:10  loss: 6.7652 (6.7652)  acc1: 53.1250 (53.1250)  acc5: 100.0000 (100.0000)  time: 1.3285  data: 1.2769  max mem: 5464\u001b[0m\n",
      "\u001b[34m[05:45:46.305687] Test:  [7/8]  eta: 0:00:00  loss: 6.7654 (6.7656)  acc1: 53.1250 (66.8142)  acc5: 100.0000 (100.0000)  time: 0.2146  data: 0.1633  max mem: 5464\u001b[0m\n",
      "\u001b[34m[05:45:46.400157] Test: Total time: 0:00:01 (0.2266 s / it)\u001b[0m\n",
      "\u001b[34m[05:45:46.400241] * Acc@1 66.814 Acc@5 100.000 loss 6.766\u001b[0m\n",
      "\u001b[34m[05:45:46.400395] Accuracy of the network on the 226 test images: 66.8%\u001b[0m\n",
      "\u001b[34m[05:45:46.400436] Max accuracy: 66.81%\u001b[0m\n",
      "\u001b[34m[05:45:46.401646] Training time 0:00:21\u001b[0m\n",
      "\u001b[34m2022-03-18 05:45:47,615 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-03-18 05:45:55 Uploading - Uploading generated training model\n",
      "2022-03-18 05:48:05 Completed - Training job completed\n",
      "Training seconds: 408\n",
      "Billable seconds: 408\n"
     ]
    }
   ],
   "source": [
    "git_config = {'repo': 'https://github.com/HaoranLv/mae.git', 'branch': 'main'}\n",
    "\n",
    "hyperparameters = {'data_path': '/opt/ml/input/data/data_path/chinesefoodnet_small', \n",
    "                   'model': 'vit_base_patch16', \n",
    "                   'finetune': '/opt/ml/input/data/weights/mae_pretrain_vit_base.pth',\n",
    "                   'output_dir': '/opt/ml/model/',\n",
    "                   'accum_iter': 4, 'batch_size': 32, 'epochs': 1, 'weight_decay': 0.05,'nb_classes':2,\n",
    "                   'drop_path': 0.1, 'mixup':0.8, 'cutmix':1.0, 'reprob':0.25, 'blr':5e-4, 'layer_decay':0.65}  # set batch and workers to 1, becasue of shared memory issue in local mode\n",
    "#                    'name': 'tutorial', 'img': 640, 'batch': 8, 'epochs': 5, 'workers': 1, 'device': '0,1,2,3,4,5,6,7'}  # set batch and workers to 1, becasue of shared memory issue in local mode\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "estimator = PyTorch(entry_point='main_finetune.py',\n",
    "                            source_dir='.',\n",
    "                            git_config=git_config,\n",
    "                            role=role,\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            framework_version='1.7.1',\n",
    "                            py_version='py36',\n",
    "                            script_mode=True,\n",
    "                            instance_count=1,  # 1 or 2 or ...\n",
    "                            instance_type=instance_type)\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-training-2022-03-18-05-39-29-920\n"
     ]
    }
   ],
   "source": [
    "# training_job_name = estimator.latest_training_job.name\n",
    "training_job_name = 'pytorch-training-2022-03-18-05-39-29-920'\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint (in this case locally) which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-847380964353/pytorch-training-2022-03-18-05-39-29-920/output/model.tar.gz to ./model.tar.gz\n",
      "checkpoint-0.pth\n",
      "log.txt\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz .\n",
    "!tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-0.pth\n",
      "code/\n",
      "code/.ipynb_checkpoints/\n",
      "code/.ipynb_checkpoints/inference-checkpoint.py\n",
      "code/.ipynb_checkpoints/models_vit-checkpoint.py\n",
      "code/models_vit.py\n",
      "code/inference.py\n",
      "code/requirements.txt\n",
      "code/__pycache__/\n",
      "code/__pycache__/models_vit.cpython-37.pyc\n",
      "upload: ./model-pytorch.tar.gz to s3://sagemaker-us-west-2-847380964353/pytorch-training-2022-03-18-05-39-29-920/output/model-pytorch.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model-pytorch.tar.gz\n",
    "# !cp checkpoint-0.pth model/\n",
    "!cd model && tar -czvf ../model-pytorch.tar.gz *\n",
    "\n",
    "!aws s3 cp model-pytorch.tar.gz s3://$bucket/$training_job_name/output/model-pytorch.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# instance_type = 'local'\n",
    "instance_type = 'ml.g4dn.xlarge'\n",
    "\n",
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='s3://{}/{}/output/model-pytorch.tar.gz'.format(bucket, training_job_name), role=role,\n",
    "                             entry_point='inference.py', framework_version='1.7.1', py_version='py36', model_server_workers=1)  # TODO set model_server_workers=1 to avoid torchhub bug\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: <class 'numpy.ndarray'> (480, 480, 3) uint8\n",
      "outputs:  (768,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "image = cv2.imread('demo/000000.jpg')\n",
    "\n",
    "print('image:', type(image), image.shape, image.dtype)\n",
    "outputs = predictor.predict(image)\n",
    "print('outputs: ', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model to SageMaker Endpoint Serverless (Preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp model-pytorch.tar.gz s3://$bucket/$training_job_name/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=6144,\n",
    "    max_concurrency=1,\n",
    ")\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sagemaker_session.boto_region_name,\n",
    "    version=\"1.9.1\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "print(image_uri)\n",
    "\n",
    "estimator = PyTorch.attach(training_job_name=training_job_name)\n",
    "serverless_predictor = estimator.deploy(serverless_inference_config=serverless_config, image_uri=image_uri, entry_point='inference.py', model_server_workers=1)\n",
    "\n",
    "# pytorch_model = PyTorchModel(model_data='s3://{}/{}/output/model-pytorch.tar.gz'.format(bucket, training_job_name), role=role,\n",
    "#                              entry_point='inference.py', framework_version='1.9.1', py_version='py38', model_server_workers=1)  # TODO set model_server_workers=1 to avoid torchhub bug\n",
    "# serverless_predictor = pytorch_model.deploy(serverless_inference_config=serverless_config, image_uri=image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread('data/images/inference/bus.jpg')\n",
    "\n",
    "# print('image:', type(image), image.shape, image.dtype)\n",
    "outputs = serverless_predictor.predict(image)\n",
    "print('outputs: ', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.delete_endpoint()\n",
    "predictor.delete_endpoint()\n",
    "serverless_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p37",
   "language": "python",
   "name": "conda_pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
